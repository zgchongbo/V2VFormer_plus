[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) 

V2VFormer++ is a Multi-Modal Vehicle-to-Vehicle Cooperative Perception framework for autonomous driving. It is also the official implementation of the <strong> TITS 2024  </strong>
paper [V2VFormer++.](https://ieeexplore.ieee.org/abstract/document/10265751)



## Citation
 If you are using our V2VFormer++ framework for your research, please cite the following paper:
 ```bibtex
@ARTICLE{yin2024v2vformer++,
  author={Yin, Hongbo and Tian, Daxin and Lin, Chunmian and Duan, Xuting and Zhou, Jianshan and Zhao, Dezong and Cao, Dongpu},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={V2VFormer++: Multi-Modal Vehicle-to-Vehicle Cooperative Perception via Global-Local Transformer}, 
  year={2024},
  volume={25},
  number={2},
  pages={2153-2166},
  keywords={Three-dimensional displays;Collaboration;Vehicular ad hoc networks;Transformers;Feature extraction;Object detection;Laser radar;Vehicle-to-vehicle (V2V) cooperative perception;multi-modal fused perception;autonomous driving;transformer;3D object detection;intelligent transportation systems},
  doi={10.1109/TITS.2023.3314919}}
```